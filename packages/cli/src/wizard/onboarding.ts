/**
 * @redbusagent/cli â€” Onboarding Wizard
 *
 * Interactive step-by-step configuration assistant using @clack/prompts.
 * Guides the user through Tier 2 (cloud LLM) and Tier 1 (local Ollama)
 * setup, then persists everything to the Vault (~/.redbusagent/config.json).
 *
 * Flow: Provider â†’ Credentials â†’ Fetch Models (live) â†’ Select Model â†’ Ollama â†’ Save
 */

import * as p from '@clack/prompts';
import pc from 'picocolors';
import { Vault, type VaultTier2Config, type VaultTier1Config, type Tier2Provider } from '@redbusagent/shared';
import { fetchTier2Models, fetchOllamaModels } from './model-fetcher.js';

// â”€â”€â”€ Wizard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export async function runOnboardingWizard(): Promise<boolean> {
    p.intro(pc.bgRed(pc.white(' ðŸ”´ redbusagent â€” Assistente de ConfiguraÃ§Ã£o ')));

    const existingConfig = Vault.read();
    if (existingConfig) {
        p.note(
            `Provedor atual: ${pc.bold(existingConfig.tier2.provider)}/${pc.bold(existingConfig.tier2.model)}\n` +
            `Vault: ${pc.dim(Vault.configPath)}`,
            'âš™ï¸  ConfiguraÃ§Ã£o existente detectada',
        );
    }

    // â”€â”€ Step 1: Tier 2 Provider â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const provider = await p.select({
        message: 'Qual LLM de nuvem (Tier 2) deseja usar?',
        options: [
            { value: 'anthropic' as const, label: 'ðŸŸ£ Anthropic (Claude)', hint: 'recomendado' },
            { value: 'google' as const, label: 'ðŸ”µ Google (Gemini)' },
            { value: 'openai' as const, label: 'ðŸŸ¢ OpenAI (GPT)' },
        ],
        initialValue: existingConfig?.tier2.provider ?? ('anthropic' as Tier2Provider),
    });
    if (p.isCancel(provider)) return false;

    // â”€â”€ Step 2: Authentication (BEFORE model selection) â”€â”€â”€â”€â”€â”€â”€â”€

    let authToken: string | undefined;
    let apiKey: string | undefined;

    if (provider === 'anthropic') {
        const key = await p.password({
            message: 'Cole sua API key do Anthropic (sk-ant-...):',
            validate: (v) => {
                if (!v || !v.startsWith('sk-ant-')) return 'API key deve comeÃ§ar com sk-ant-';
            },
        });
        if (p.isCancel(key)) return false;
        apiKey = key.trim();
    } else {
        const keyLabel = provider === 'google'
            ? 'Cole sua Google AI API key:'
            : 'Cole sua OpenAI API key (sk-...):';

        const key = await p.password({
            message: keyLabel,
            validate: (v) => {
                if (!v || v.trim().length < 10) return 'Chave invÃ¡lida.';
            },
        });
        if (p.isCancel(key)) return false;
        apiKey = key.trim();
    }

    // â”€â”€ Step 3: Fetch Models (dynamic!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const s = p.spinner();
    s.start(`Buscando modelos disponÃ­veis no ${provider}...`);

    const fetchResult = await fetchTier2Models(provider, { apiKey, authToken });

    if (fetchResult.usingFallback) {
        s.stop(pc.yellow(`âš ï¸  NÃ£o foi possÃ­vel listar modelos (${fetchResult.error ?? 'erro'}) â€” usando lista padrÃ£o`));
    } else {
        s.stop(pc.green(`${fetchResult.models.length} modelos encontrados!`));
    }

    if (fetchResult.models.length === 0) {
        p.log.error('Nenhum modelo disponÃ­vel. Verifique sua credencial.');
        return false;
    }

    // â”€â”€ Step 4: Model Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const model = await p.select({
        message: `Qual modelo do ${provider} deseja usar?`,
        options: fetchResult.models.map(m => ({
            value: m.id,
            label: m.label,
            hint: m.hint,
        })),
    });
    if (p.isCancel(model)) return false;

    const tier2Config: VaultTier2Config = {
        provider,
        model,
        ...(authToken ? { authToken } : {}),
        ...(apiKey ? { apiKey } : {}),
    };

    // â”€â”€ Step 5: Tier 1 (Ollama Local) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const configureTier1 = await p.confirm({
        message: 'Deseja configurar um modelo local via Ollama (Tier 1)?',
        initialValue: true,
    });
    if (p.isCancel(configureTier1)) return false;

    let tier1Config: VaultTier1Config;

    if (configureTier1) {
        const ollamaUrl = await p.text({
            message: 'URL do Ollama:',
            placeholder: 'http://127.0.0.1:11434',
            defaultValue: 'http://127.0.0.1:11434',
            initialValue: existingConfig?.tier1?.url ?? 'http://127.0.0.1:11434',
        });
        if (p.isCancel(ollamaUrl)) return false;

        // Try to fetch Ollama models dynamically too
        const ollamaSpinner = p.spinner();
        ollamaSpinner.start('Buscando modelos locais no Ollama...');

        const ollamaResult = await fetchOllamaModels(ollamaUrl);

        if (ollamaResult.success && ollamaResult.models.length > 0) {
            ollamaSpinner.stop(pc.green(`${ollamaResult.models.length} modelos locais encontrados!`));

            const ollamaModel = await p.select({
                message: 'Qual modelo local usar?',
                options: ollamaResult.models.map(m => ({
                    value: m.id,
                    label: m.label,
                    hint: m.hint,
                })),
            });
            if (p.isCancel(ollamaModel)) return false;

            tier1Config = { enabled: true, url: ollamaUrl, model: ollamaModel };
        } else {
            ollamaSpinner.stop(pc.yellow('Ollama nÃ£o encontrado â€” digite o nome do modelo manualmente'));

            const manualModel = await p.text({
                message: 'Nome do modelo Ollama:',
                placeholder: 'llama3',
                defaultValue: 'llama3',
                initialValue: existingConfig?.tier1?.model ?? 'llama3',
            });
            if (p.isCancel(manualModel)) return false;

            tier1Config = { enabled: true, url: ollamaUrl, model: manualModel };
        }
    } else {
        tier1Config = { enabled: false, url: 'http://127.0.0.1:11434', model: 'llama3' };
    }

    // â”€â”€ Step 6: Save to Vault â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const saveSpinner = p.spinner();
    saveSpinner.start('Salvando configuraÃ§Ã£o no Cofre...');

    Vault.write({
        version: Vault.schemaVersion,
        tier2: tier2Config,
        tier1: tier1Config,
    });

    await new Promise(r => setTimeout(r, 500));
    saveSpinner.stop('ConfiguraÃ§Ã£o salva!');

    p.note(
        `Provedor: ${pc.bold(tier2Config.provider)}/${pc.bold(tier2Config.model)}\n` +
        `Auth: ${pc.bold(tier2Config.authToken ? 'OAuth token' : 'API key')}\n` +
        `Ollama: ${pc.bold(tier1Config.enabled ? `${tier1Config.model} @ ${tier1Config.url}` : 'desativado')}\n` +
        `Vault: ${pc.dim(Vault.configPath)}`,
        'âœ… Resumo da configuraÃ§Ã£o',
    );

    p.outro(pc.green('ConfiguraÃ§Ã£o concluÃ­da! Rode: ') + pc.bold(pc.cyan('redbus start')));

    return true;
}
